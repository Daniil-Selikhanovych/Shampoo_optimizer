{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jw3lv6hZccRk",
    "outputId": "506246e7-7158-4a1c-920e-912b59d3cfb3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from matrix_square_root_power import *\n",
    "from shampoo_optimizer import *\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import tensorflow as tf\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation, Dropout\n",
    "from keras.layers import AveragePooling2D, Input, Flatten, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model, Sequential\n",
    "from keras.datasets import cifar10\n",
    "from keras import backend\n",
    "from keras.utils import np_utils\n",
    "import numpy\n",
    "from keras.datasets import cifar10\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "2R-JY270ccRo",
    "outputId": "ccdd2fbc-6c5b-4317-bf40-98fdecdcf43e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IQWmnT2yccRs"
   },
   "outputs": [],
   "source": [
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vfwv7lxZccRu"
   },
   "outputs": [],
   "source": [
    "def loss(model, x, y, loss_object):\n",
    "    y_ = model(x)\n",
    "\n",
    "    return loss_object(y_true=y, y_pred=y_)\n",
    "\n",
    "def grad(model, inputs, targets, loss_object):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets, loss_object)\n",
    "        return loss_value, tape.gradient(loss_value, model.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xJR8Jck7ccRx"
   },
   "outputs": [],
   "source": [
    "train_images = tf.convert_to_tensor(train_images, dtype=tf.float32)\n",
    "test_images = tf.convert_to_tensor(test_images, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "id": "5lNy8WioccRz",
    "outputId": "131130d7-3be4-4ee6-bce0-a622927632e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: Loss: 2.192, Accuracy: 28.726%\n",
      "Epoch 001: Loss: 2.124, Accuracy: 36.370%\n",
      "Epoch 002: Loss: 2.094, Accuracy: 39.391%\n",
      "Epoch 003: Loss: 2.072, Accuracy: 41.705%\n",
      "Epoch 004: Loss: 2.054, Accuracy: 43.582%\n",
      "Epoch 005: Loss: 2.040, Accuracy: 45.088%\n",
      "Epoch 006: Loss: 2.029, Accuracy: 46.086%\n",
      "Epoch 007: Loss: 2.020, Accuracy: 46.951%\n",
      "Epoch 008: Loss: 2.013, Accuracy: 47.650%\n",
      "Epoch 009: Loss: 2.006, Accuracy: 48.171%\n",
      "Epoch 010: Loss: 2.001, Accuracy: 48.598%\n",
      "Epoch 011: Loss: 1.996, Accuracy: 49.052%\n",
      "Epoch 012: Loss: 1.992, Accuracy: 49.463%\n",
      "Epoch 013: Loss: 1.988, Accuracy: 49.796%\n",
      "Epoch 014: Loss: 1.984, Accuracy: 50.152%\n",
      "Epoch 015: Loss: 1.981, Accuracy: 50.451%\n",
      "Epoch 016: Loss: 1.977, Accuracy: 50.701%\n",
      "Epoch 017: Loss: 1.974, Accuracy: 50.929%\n",
      "Epoch 018: Loss: 1.972, Accuracy: 51.240%\n",
      "Epoch 019: Loss: 1.969, Accuracy: 51.468%\n",
      "Epoch 020: Loss: 1.967, Accuracy: 51.747%\n",
      "Epoch 021: Loss: 1.964, Accuracy: 51.941%\n",
      "Epoch 022: Loss: 1.962, Accuracy: 52.137%\n",
      "Epoch 023: Loss: 1.960, Accuracy: 52.354%\n",
      "Epoch 024: Loss: 1.958, Accuracy: 52.568%\n",
      "Epoch 025: Loss: 1.956, Accuracy: 52.774%\n",
      "Epoch 026: Loss: 1.954, Accuracy: 52.979%\n",
      "Epoch 027: Loss: 1.952, Accuracy: 53.157%\n",
      "Epoch 028: Loss: 1.950, Accuracy: 53.321%\n",
      "Epoch 029: Loss: 1.949, Accuracy: 53.478%\n",
      "Epoch 030: Loss: 1.947, Accuracy: 53.656%\n",
      "Epoch 031: Loss: 1.946, Accuracy: 53.814%\n",
      "Epoch 032: Loss: 1.944, Accuracy: 53.934%\n",
      "Epoch 033: Loss: 1.943, Accuracy: 54.075%\n",
      "Epoch 034: Loss: 1.941, Accuracy: 54.213%\n",
      "Epoch 035: Loss: 1.940, Accuracy: 54.341%\n",
      "Epoch 036: Loss: 1.939, Accuracy: 54.497%\n",
      "Epoch 037: Loss: 1.937, Accuracy: 54.621%\n",
      "Epoch 038: Loss: 1.936, Accuracy: 54.760%\n",
      "Epoch 039: Loss: 1.935, Accuracy: 54.874%\n",
      "Epoch 040: Loss: 1.934, Accuracy: 55.024%\n",
      "Epoch 041: Loss: 1.932, Accuracy: 55.146%\n",
      "Epoch 042: Loss: 1.931, Accuracy: 55.242%\n",
      "Epoch 043: Loss: 1.930, Accuracy: 55.323%\n",
      "Epoch 044: Loss: 1.929, Accuracy: 55.365%\n",
      "Epoch 045: Loss: 1.928, Accuracy: 55.439%\n",
      "Epoch 046: Loss: 1.927, Accuracy: 55.537%\n",
      "Epoch 047: Loss: 1.926, Accuracy: 55.635%\n",
      "Epoch 048: Loss: 1.925, Accuracy: 55.761%\n",
      "Epoch 049: Loss: 1.924, Accuracy: 55.875%\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = ShampooOptimizer(learning_rate=0.01)\n",
    "\n",
    "shampoo_train_loss_results = []\n",
    "shampoo_train_accuracy_results = []\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "    epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "    # Training loop - using batches of 128\n",
    "    for i in range(train_images.shape[0] // batch_size):\n",
    "        x = train_images[i*batch_size:i*batch_size+batch_size]\n",
    "        y = train_labels[i*batch_size:i*batch_size+batch_size]\n",
    "        # Optimize the model\n",
    "        loss_value, grads = grad(model, x, y, loss_object)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Track progress\n",
    "        epoch_loss_avg(loss_value)  # Add current batch loss\n",
    "        # Compare predicted label to actual label\n",
    "        epoch_accuracy(y, model(x))\n",
    "\n",
    "        # End epoch\n",
    "    shampoo_train_loss_results.append(epoch_loss_avg.result())\n",
    "    shampoo_train_accuracy_results.append(epoch_accuracy.result())\n",
    "\n",
    "    print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,\n",
    "                                                                epoch_loss_avg.result(),\n",
    "                                                                epoch_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "8ddXXfJ9tyIH",
    "outputId": "52c8d85e-b74a-49f3-d6b7-ad065cbaa4f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.1.0-rc1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/97/fbec42dfdb93a37ec971ca0996ff70b8eb5817789a9c1880aafd4684c9af/tensorflow-2.1.0rc1-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
      "\u001b[K     |████████████████████████████████| 421.8MB 38kB/s \n",
      "\u001b[?25hRequirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc1) (0.2.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc1) (0.8.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc1) (1.0.8)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc1) (1.11.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc1) (1.15.0)\n",
      "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
      "\u001b[K     |████████████████████████████████| 450kB 38.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc1) (0.1.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc1) (1.1.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc1) (0.8.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc1) (1.17.4)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc1) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc1) (3.10.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc1) (0.33.6)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc1) (1.12.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc1) (3.1.0)\n",
      "Collecting tensorboard<2.2.0,>=2.1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/23/53ffe290341cd0855d595b0a2e7485932f473798af173bbe3a584b99bb06/tensorboard-2.1.0-py3-none-any.whl (3.8MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8MB 37.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.1.0-rc1) (2.8.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow==2.1.0-rc1) (42.0.2)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/f8/84b5771faec3eba9fe0c91c8c5896364a8ba08852c0dea5ad2025026dd95/google_auth-1.10.0-py2.py3-none-any.whl (76kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 12.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc1) (0.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc1) (2.21.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc1) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc1) (0.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc1) (0.2.7)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc1) (4.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc1) (3.1.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc1) (1.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc1) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc1) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc1) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc1) (2.8)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc1) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc1) (3.1.0)\n",
      "\u001b[31mERROR: tensorboard 2.1.0 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.10.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: tensorflow-estimator, google-auth, tensorboard, tensorflow\n",
      "  Found existing installation: tensorflow-estimator 1.15.1\n",
      "    Uninstalling tensorflow-estimator-1.15.1:\n",
      "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
      "  Found existing installation: google-auth 1.4.2\n",
      "    Uninstalling google-auth-1.4.2:\n",
      "      Successfully uninstalled google-auth-1.4.2\n",
      "  Found existing installation: tensorboard 1.15.0\n",
      "    Uninstalling tensorboard-1.15.0:\n",
      "      Successfully uninstalled tensorboard-1.15.0\n",
      "  Found existing installation: tensorflow 1.15.0\n",
      "    Uninstalling tensorflow-1.15.0:\n",
      "      Successfully uninstalled tensorflow-1.15.0\n",
      "Successfully installed google-auth-1.10.0 tensorboard-2.1.0 tensorflow-2.1.0rc1 tensorflow-estimator-2.1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "google",
         "tensorboard",
         "tensorflow",
         "tensorflow_core",
         "tensorflow_estimator"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install tensorflow==2.1.0-rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KPF_b3UatmO2",
    "outputId": "056d736a-7f31-4abb-ae3e-2279b7766505"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "#download mnist data and split into train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "30FEKX54u-ez"
   },
   "outputs": [],
   "source": [
    "#reshape data to fit model\n",
    "X_train = X_train.reshape(60000,28,28,1)\n",
    "X_test = X_test.reshape(10000,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "FcbVuvfkvDzh",
    "outputId": "9c299ee1-016d-4724-d0e5-760851a46509"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 32)        18464     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 18432)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                184330    \n",
      "=================================================================\n",
      "Total params: 203,434\n",
      "Trainable params: 203,434\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "#create model\n",
    "model = Sequential()\n",
    "#add model layers\n",
    "model.add(Conv2D(64, kernel_size=3, activation=\"relu\", input_shape=(28,28,1)))\n",
    "model.add(Conv2D(32, kernel_size=3, activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HSteRsubvJ6c"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vt4Ct2mtvT8x"
   },
   "outputs": [],
   "source": [
    "def loss(model, x, y, loss_object):\n",
    "    y_ = model(x)\n",
    "\n",
    "    return loss_object(y_true=y, y_pred=y_)\n",
    "\n",
    "def grad(model, inputs, targets, loss_object):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets, loss_object)\n",
    "        return loss_value, tape.gradient(loss_value, model.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "colab_type": "code",
    "id": "qCRcJR3ceoeB",
    "outputId": "1ae163d3-c5a8-4526-fff6-862f4911c85a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 24, 24, 32)        18464     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 18432)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                184330    \n",
      "=================================================================\n",
      "Total params: 203,434\n",
      "Trainable params: 203,434\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 000: Loss: 2.015, Accuracy: 45.059%\n",
      "Epoch 001: Loss: 1.962, Accuracy: 49.968%\n",
      "Epoch 002: Loss: 1.943, Accuracy: 51.831%\n",
      "Epoch 003: Loss: 1.929, Accuracy: 53.188%\n",
      "Epoch 004: Loss: 1.922, Accuracy: 53.863%\n",
      "Epoch 005: Loss: 1.918, Accuracy: 54.262%\n",
      "Epoch 006: Loss: 1.915, Accuracy: 54.591%\n",
      "Epoch 007: Loss: 1.912, Accuracy: 54.816%\n",
      "Epoch 008: Loss: 1.910, Accuracy: 55.026%\n",
      "Epoch 009: Loss: 1.909, Accuracy: 55.178%\n",
      "Epoch 010: Loss: 1.907, Accuracy: 55.312%\n",
      "Epoch 011: Loss: 1.906, Accuracy: 55.439%\n",
      "Epoch 012: Loss: 1.905, Accuracy: 55.532%\n",
      "Epoch 013: Loss: 1.904, Accuracy: 55.629%\n",
      "Epoch 014: Loss: 1.903, Accuracy: 55.731%\n",
      "Epoch 015: Loss: 1.902, Accuracy: 55.823%\n",
      "Epoch 016: Loss: 1.902, Accuracy: 55.893%\n",
      "Epoch 017: Loss: 1.901, Accuracy: 55.950%\n",
      "Epoch 018: Loss: 1.900, Accuracy: 56.025%\n",
      "Epoch 019: Loss: 1.900, Accuracy: 56.068%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "model_shampoo_use_iterative_root = Sequential()\n",
    "#add model layers\n",
    "model_shampoo_use_iterative_root.add(Conv2D(64, kernel_size=3, activation=\"relu\", input_shape=(28,28,1)))\n",
    "model_shampoo_use_iterative_root.add(Conv2D(32, kernel_size=3, activation=\"relu\"))\n",
    "model_shampoo_use_iterative_root.add(Flatten())\n",
    "model_shampoo_use_iterative_root.add(Dense(10, activation=\"softmax\"))\n",
    "model_shampoo_use_iterative_root.summary()\n",
    "shampoo_use_iterative_root_loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "shampoo_use_iterative_root_optimizer = ShampooOptimizer(learning_rate=5*1e-5)\n",
    "\n",
    "shampoo_use_iterative_root_train_loss_results = []\n",
    "shampoo_use_iterative_root_train_accuracy_results = []\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 128\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "    epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "    # Training loop - using batches of 128\n",
    "    for i in range(X_train.shape[0] // batch_size):\n",
    "        x = X_train[i*batch_size:i*batch_size+batch_size]\n",
    "        y = y_train[i*batch_size:i*batch_size+batch_size]\n",
    "        # Optimize the model\n",
    "        loss_value, grads = grad(model_shampoo_use_iterative_root, x, y, shampoo_use_iterative_root_loss_object)\n",
    "        shampoo_use_iterative_root_optimizer.apply_gradients(zip(grads, model_shampoo_use_iterative_root.trainable_weights))\n",
    "\n",
    "        # Track progress\n",
    "        epoch_loss_avg(loss_value)  # Add current batch loss\n",
    "        # Compare predicted label to actual label\n",
    "        epoch_accuracy(y, model_shampoo_use_iterative_root(x))\n",
    "\n",
    "        # End epoch\n",
    "    shampoo_use_iterative_root_train_loss_results.append(epoch_loss_avg.result())\n",
    "    shampoo_use_iterative_root_train_accuracy_results.append(epoch_accuracy.result())\n",
    "\n",
    "    print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,\n",
    "                                                                epoch_loss_avg.result(),\n",
    "                                                                epoch_accuracy.result()))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "CIFAR10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
