{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8cREZFcZc__W",
    "outputId": "4ef5eb0b-8e11-44af-ca05-76891d81d176"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from matrix_square_root_power import *\n",
    "from shampoo_optimizer import *\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import tensorflow as tf\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation, Dropout\n",
    "from keras.layers import AveragePooling2D, Input, Flatten, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model, Sequential\n",
    "from keras.datasets import cifar10\n",
    "from keras import backend\n",
    "from keras.utils import np_utils\n",
    "import numpy\n",
    "from keras.datasets import cifar10\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9NbrXOnjc__a"
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zu6U4-tdc__d"
   },
   "outputs": [],
   "source": [
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p4jBipOxc__g"
   },
   "outputs": [],
   "source": [
    "def loss(model, x, y, loss_object):\n",
    "    y_ = model(x)\n",
    "\n",
    "    return loss_object(y_true=y, y_pred=y_)\n",
    "\n",
    "def grad(model, inputs, targets, loss_object):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets, loss_object)\n",
    "        return loss_value, tape.gradient(loss_value, model.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oVnLzPYAc__j"
   },
   "outputs": [],
   "source": [
    "train_images = tf.convert_to_tensor(train_images, dtype=tf.float32)\n",
    "test_images = tf.convert_to_tensor(test_images, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9W7yNKNfh08"
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "id": "IP3Cv2Tpc__m",
    "outputId": "d150d739-d0bd-4760-a054-b9b9f228ff08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: Loss: 2.134, Accuracy: 32.957%\n",
      "Epoch 001: Loss: 2.035, Accuracy: 43.606%\n",
      "Epoch 002: Loss: 1.983, Accuracy: 48.870%\n",
      "Epoch 003: Loss: 1.945, Accuracy: 53.017%\n",
      "Epoch 004: Loss: 1.921, Accuracy: 55.663%\n",
      "Epoch 005: Loss: 1.893, Accuracy: 58.468%\n",
      "Epoch 006: Loss: 1.878, Accuracy: 60.006%\n",
      "Epoch 007: Loss: 1.864, Accuracy: 61.432%\n",
      "Epoch 008: Loss: 1.851, Accuracy: 62.786%\n",
      "Epoch 009: Loss: 1.837, Accuracy: 64.151%\n",
      "Epoch 010: Loss: 1.827, Accuracy: 65.182%\n",
      "Epoch 011: Loss: 1.815, Accuracy: 66.464%\n",
      "Epoch 012: Loss: 1.807, Accuracy: 67.175%\n",
      "Epoch 013: Loss: 1.796, Accuracy: 68.353%\n",
      "Epoch 014: Loss: 1.791, Accuracy: 68.760%\n",
      "Epoch 015: Loss: 1.782, Accuracy: 69.804%\n",
      "Epoch 016: Loss: 1.777, Accuracy: 70.246%\n",
      "Epoch 017: Loss: 1.770, Accuracy: 70.933%\n",
      "Epoch 018: Loss: 1.764, Accuracy: 71.534%\n",
      "Epoch 019: Loss: 1.753, Accuracy: 72.588%\n",
      "Epoch 020: Loss: 1.752, Accuracy: 72.692%\n",
      "Epoch 021: Loss: 1.745, Accuracy: 73.369%\n",
      "Epoch 022: Loss: 1.739, Accuracy: 73.992%\n",
      "Epoch 023: Loss: 1.734, Accuracy: 74.543%\n",
      "Epoch 024: Loss: 1.730, Accuracy: 74.844%\n",
      "Epoch 025: Loss: 1.727, Accuracy: 75.214%\n",
      "Epoch 026: Loss: 1.721, Accuracy: 75.731%\n",
      "Epoch 027: Loss: 1.720, Accuracy: 75.847%\n",
      "Epoch 028: Loss: 1.715, Accuracy: 76.288%\n",
      "Epoch 029: Loss: 1.712, Accuracy: 76.651%\n",
      "Epoch 030: Loss: 1.710, Accuracy: 76.833%\n",
      "Epoch 031: Loss: 1.706, Accuracy: 77.165%\n",
      "Epoch 032: Loss: 1.706, Accuracy: 77.260%\n",
      "Epoch 033: Loss: 1.704, Accuracy: 77.408%\n",
      "Epoch 034: Loss: 1.697, Accuracy: 78.069%\n",
      "Epoch 035: Loss: 1.700, Accuracy: 77.871%\n",
      "Epoch 036: Loss: 1.695, Accuracy: 78.259%\n",
      "Epoch 037: Loss: 1.691, Accuracy: 78.658%\n",
      "Epoch 038: Loss: 1.688, Accuracy: 78.840%\n",
      "Epoch 039: Loss: 1.687, Accuracy: 78.934%\n",
      "Epoch 040: Loss: 1.682, Accuracy: 79.465%\n",
      "Epoch 041: Loss: 1.682, Accuracy: 79.511%\n",
      "Epoch 042: Loss: 1.680, Accuracy: 79.623%\n",
      "Epoch 043: Loss: 1.677, Accuracy: 79.906%\n",
      "Epoch 044: Loss: 1.674, Accuracy: 80.319%\n",
      "Epoch 045: Loss: 1.670, Accuracy: 80.501%\n",
      "Epoch 046: Loss: 1.668, Accuracy: 80.783%\n",
      "Epoch 047: Loss: 1.669, Accuracy: 80.805%\n",
      "Epoch 048: Loss: 1.665, Accuracy: 80.970%\n",
      "Epoch 049: Loss: 1.664, Accuracy: 81.180%\n"
     ]
    }
   ],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "shampoo_train_loss_results = []\n",
    "shampoo_train_accuracy_results = []\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "    epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "    # Training loop - using batches of 128\n",
    "    for i in range(train_images.shape[0] // batch_size):\n",
    "        x = train_images[i*batch_size:i*batch_size+batch_size]\n",
    "        y = train_labels[i*batch_size:i*batch_size+batch_size]\n",
    "        # Optimize the model\n",
    "        loss_value, grads = grad(model, x, y, loss_object)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Track progress\n",
    "        epoch_loss_avg(loss_value)  # Add current batch loss\n",
    "        # Compare predicted label to actual label\n",
    "        epoch_accuracy(y, model(x))\n",
    "\n",
    "        # End epoch\n",
    "    shampoo_train_loss_results.append(epoch_loss_avg.result())\n",
    "    shampoo_train_accuracy_results.append(epoch_accuracy.result())\n",
    "\n",
    "    print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,\n",
    "                                                                epoch_loss_avg.result(),\n",
    "                                                                epoch_accuracy.result()))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "2_5463056889929532830.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
