{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "name": "MNIST.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NM46j3O1nM-P",
        "colab_type": "code",
        "outputId": "9c03dda3-21b7-4c0d-c791-f360ca757bea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install tensorflow==2.1.0-rc1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.1.0-rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/97/fbec42dfdb93a37ec971ca0996ff70b8eb5817789a9c1880aafd4684c9af/tensorflow-2.1.0rc1-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 35kB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc1) (0.1.8)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc1) (3.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc1) (0.8.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc1) (1.11.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc1) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc1) (1.0.8)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 49.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc1) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc1) (0.8.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc1) (0.2.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc1) (3.10.0)\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/23/53ffe290341cd0855d595b0a2e7485932f473798af173bbe3a584b99bb06/tensorboard-2.1.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 39.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc1) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc1) (0.33.6)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc1) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc1) (1.17.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.1.0-rc1) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow==2.1.0-rc1) (42.0.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc1) (0.16.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc1) (2.21.0)\n",
            "Collecting google-auth<2,>=1.6.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/83/3cb31033e1ea0bdb8991b6ef327a5bf4960bd3dd31ff355881bfb0ddf199/google_auth-1.9.0-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc1) (3.1.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc1) (0.4.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc1) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc1) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc1) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc1) (2019.11.28)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc1) (3.1.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc1) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc1) (0.2.7)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc1) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc1) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0-rc1) (3.1.0)\n",
            "\u001b[31mERROR: tensorboard 2.1.0 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.9.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, google-auth, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: google-auth 1.4.2\n",
            "    Uninstalling google-auth-1.4.2:\n",
            "      Successfully uninstalled google-auth-1.4.2\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed google-auth-1.9.0 tensorboard-2.1.0 tensorflow-2.1.0rc1 tensorflow-estimator-2.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HPBJjqonJWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from tensorflow.python.framework import dtypes\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.platform import tf_logging\n",
        "from tensorflow.python.training import optimizer\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.ops import state_ops\n",
        "from tensorflow.python.ops import control_flow_ops\n",
        "from tensorflow.python.ops import linalg_ops\n",
        "from tensorflow.python.ops import math_ops\n",
        "\n",
        "\n",
        "\n",
        "def matrix_square_root(mat_a, mat_a_size, iter_count=100, ridge_epsilon=1e-4):\n",
        "    \"\"\"Iterative method to get matrix square root.\n",
        "\n",
        "    Stable iterations for the matrix square root, Nicholas J. Higham\n",
        "\n",
        "    Page 231, Eq 2.6b\n",
        "    http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.6.8799&rep=rep1&type=pdf\n",
        "\n",
        "    Args:\n",
        "        mat_a: the symmetric PSD matrix whose matrix square root be computed\n",
        "        mat_a_size: size of mat_a.\n",
        "        iter_count: Maximum number of iterations.\n",
        "        ridge_epsilon: Ridge epsilon added to make the matrix positive definite.\n",
        "\n",
        "    Returns:\n",
        "        mat_a^0.5\n",
        "    \"\"\"\n",
        "\n",
        "    def _iter_condition(i, unused_mat_y, unused_old_mat_y, unused_mat_z,\n",
        "                      unused_old_mat_z, err, old_err):\n",
        "        # This method require that we check for divergence every step.\n",
        "        return math_ops.logical_and(i < iter_count, err < old_err)\n",
        "\n",
        "    def _iter_body(i, mat_y, unused_old_mat_y, mat_z, unused_old_mat_z, err,\n",
        "                 unused_old_err):\n",
        "        current_iterate = 0.5 * (3.0 * identity - math_ops.matmul(mat_z, mat_y))\n",
        "        current_mat_y = math_ops.matmul(mat_y, current_iterate)\n",
        "        current_mat_z = math_ops.matmul(current_iterate, mat_z)\n",
        "        # Compute the error in approximation.\n",
        "        mat_sqrt_a = current_mat_y * math_ops.sqrt(norm)\n",
        "        mat_a_approx = math_ops.matmul(mat_sqrt_a, mat_sqrt_a)\n",
        "        residual = mat_a - mat_a_approx\n",
        "        current_err = math_ops.sqrt(math_ops.reduce_sum(residual * residual)) / norm\n",
        "        return i + 1, current_mat_y, mat_y, current_mat_z, mat_z, current_err, err\n",
        "\n",
        "    identity = linalg_ops.eye(math_ops.cast(mat_a_size, dtypes.int32))\n",
        "    mat_a = mat_a + ridge_epsilon * identity\n",
        "    norm = math_ops.sqrt(math_ops.reduce_sum(mat_a * mat_a))\n",
        "    mat_init_y = mat_a / norm\n",
        "    mat_init_z = identity\n",
        "    init_err = norm\n",
        "\n",
        "    _, _, prev_mat_y, _, _, _, _ = control_flow_ops.while_loop(\n",
        "      _iter_condition, _iter_body, [\n",
        "          0, mat_init_y, mat_init_y, mat_init_z, mat_init_z, init_err,\n",
        "          init_err + 1.0\n",
        "      ])\n",
        "    return prev_mat_y * math_ops.sqrt(norm)\n",
        "\n",
        "\n",
        "def matrix_inverse_pth_root(mat_g,\n",
        "                            mat_g_size,\n",
        "                            alpha,\n",
        "                            iter_count=100,\n",
        "                            epsilon=1e-6,\n",
        "                            ridge_epsilon=1e-6):\n",
        "    \"\"\"Computes mat_g^alpha, where alpha = -1/p, p a positive integer.\n",
        "\n",
        "    We use an iterative Schur-Newton method from equation 3.2 on page 9 of:\n",
        "\n",
        "    A Schur-Newton Method for the Matrix p-th Root and its Inverse\n",
        "    by Chun-Hua Guo and Nicholas J. Higham\n",
        "    SIAM Journal on Matrix Analysis and Applications,\n",
        "    2006, Vol. 28, No. 3 : pp. 788-804\n",
        "    https://pdfs.semanticscholar.org/0abe/7f77433cf5908bfe2b79aa91af881da83858.pdf\n",
        "\n",
        "    Args:\n",
        "        mat_g: the symmetric PSD matrix whose power it to be computed\n",
        "        mat_g_size: size of mat_g.\n",
        "        alpha: exponent, must be -1/p for p a positive integer.\n",
        "        iter_count: Maximum number of iterations.\n",
        "        epsilon: accuracy indicator, useful for early termination.\n",
        "        ridge_epsilon: Ridge epsilon added to make the matrix positive definite.\n",
        "\n",
        "    Returns:\n",
        "        mat_g^alpha\n",
        "    \"\"\"\n",
        "\n",
        "    identity = linalg_ops.eye(math_ops.cast(mat_g_size, dtypes.int32))\n",
        "\n",
        "    def mat_power(mat_m, p):\n",
        "        \"\"\"Computes mat_m^p, for p a positive integer.\n",
        "\n",
        "        Power p is known at graph compile time, so no need for loop and cond.\n",
        "        Args:\n",
        "            mat_m: a square matrix\n",
        "            p: a positive integer\n",
        "\n",
        "        Returns:\n",
        "          mat_m^p\n",
        "        \"\"\"\n",
        "        assert p == int(p) and p > 0\n",
        "        power = None\n",
        "        while p > 0:\n",
        "            if p % 2 == 1:\n",
        "                power = math_ops.matmul(mat_m, power) if power is not None else mat_m\n",
        "            p //= 2\n",
        "            mat_m = math_ops.matmul(mat_m, mat_m)\n",
        "        return power\n",
        "\n",
        "    def _iter_condition(i, mat_m, _):\n",
        "        return math_ops.logical_and(\n",
        "            i < iter_count,\n",
        "            math_ops.reduce_max(math_ops.abs(mat_m - identity)) > epsilon)\n",
        "\n",
        "    def _iter_body(i, mat_m, mat_x):\n",
        "        mat_m_i = (1 - alpha) * identity + alpha * mat_m\n",
        "        return (i + 1, math_ops.matmul(mat_power(mat_m_i, -1.0 / alpha), mat_m),\n",
        "            math_ops.matmul(mat_x, mat_m_i))\n",
        "\n",
        "    if mat_g_size == 1:\n",
        "        mat_h = math_ops.pow(mat_g + ridge_epsilon, alpha)\n",
        "    else:\n",
        "        damped_mat_g = mat_g + ridge_epsilon * identity\n",
        "        z = (1 - 1 / alpha) / (2 * linalg_ops.norm(damped_mat_g))\n",
        "        # The best value for z is\n",
        "        # (1 - 1/alpha) * (c_max^{-alpha} - c_min^{-alpha}) /\n",
        "        #                 (c_max^{1-alpha} - c_min^{1-alpha})\n",
        "        # where c_max and c_min are the largest and smallest singular values of\n",
        "        # damped_mat_g.\n",
        "        # The above estimate assumes that c_max > c_min * 2^p. (p = -1/alpha)\n",
        "        # Can replace above line by the one below, but it is less accurate,\n",
        "        # hence needs more iterations to converge.\n",
        "        # z = (1 - 1/alpha) / math_ops.trace(damped_mat_g)\n",
        "        # If we want the method to always converge, use z = 1 / norm(damped_mat_g)\n",
        "        # or z = 1 / math_ops.trace(damped_mat_g), but these can result in many\n",
        "        # extra iterations.\n",
        "        _, _, mat_h = control_flow_ops.while_loop(\n",
        "            _iter_condition, _iter_body,\n",
        "            [0, damped_mat_g * z, identity * math_ops.pow(z, -alpha)])\n",
        "    return mat_h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45T4ooF7nJW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"The Shampoo Optimizer.\n",
        "\n",
        "Variant of Adagrad using one preconditioner matrix per variable dimension.\n",
        "For details, see https://arxiv.org/abs/1802.09568\n",
        "\"\"\"\n",
        "def GetParam(var, timestep):\n",
        "    if callable(var):\n",
        "        return var(timestep)\n",
        "    else:\n",
        "        return var\n",
        "\n",
        "\n",
        "class ShampooOptimizer(optimizer.Optimizer):\n",
        "    \"\"\"The Shampoo Optimizer\n",
        "\n",
        "    Variant of Adagrad using one preconditioner matrix per variable dimension.\n",
        "    For details, see https://arxiv.org/abs/1802.09568\n",
        "\n",
        "    gbar is time-weighted accumulated gradient:\n",
        "    gbar[t] = gbar_decay[t] * gbar[t-1] + gbar_weight[t] * g[t]\n",
        "\n",
        "    mat_gbar is time-weighted accumulated gradient square:\n",
        "    mat_gbar_j[t] = mat_gbar_decay[t] * mat_gbar_j[t-1]\n",
        "                  + mat_gbar_weight[t] * gg_j[t]\n",
        "    where if g[t] = g_abcd then gg_a[t] = g_abcd g_a'bcd (Einstein notation)\n",
        "\n",
        "    Update rule:\n",
        "    w[t+1] = w[t] - learning_rate[t] * Prod_j mat_gbar_j[t]^(-alpha/n) gbar[t]\n",
        "    Again, mat_gbar_j[t]^(-alpha) gbar[t] is a tensor contraction along the\n",
        "    j'th dimension of gbar[t] with the first dimension of\n",
        "    mat_gbar_j[t]^(-alpha/n), where alpha is a hyperparameter,\n",
        "    and n = rank of the variable.\n",
        "    Prod_j represents doing this contraction for all j in 0..n-1.\n",
        "\n",
        "    Typically learning_rate is constant, but could be time dependent by passing\n",
        "    a lambda function that depends on step.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "               global_step=0,\n",
        "               max_matrix_size=768,\n",
        "               gbar_decay=0.0,\n",
        "               gbar_weight=1.0,\n",
        "               mat_gbar_decay=1.0,\n",
        "               mat_gbar_weight=1.0,\n",
        "               learning_rate=1.0,\n",
        "               svd_interval=1,\n",
        "               precond_update_interval=1,\n",
        "               epsilon=1e-4,\n",
        "               alpha=0.5,\n",
        "               use_iterative_root=False,\n",
        "               use_locking=False,\n",
        "               name=\"Shampoo\"):\n",
        "        \"\"\"Default values of the various hyper-parameters.\n",
        "\n",
        "        gbar_decay, gbar_weight etc. can be a float or a time varying parameter.\n",
        "        For time-varying parameters use e.g. \"lambda T: T / (T + 1.0)\"\n",
        "        where the expression in the lambda is a tensorflow expression\n",
        "\n",
        "        Args:\n",
        "          global_step: tensorflow variable indicating the step.\n",
        "          max_matrix_size: We do not perform SVD for matrices larger than this.\n",
        "          gbar_decay:\n",
        "          gbar_weight:  Used to update gbar:\n",
        "                gbar[t] = gbar_decay[t] * gbar[t-1] + gbar_weight[t] * g[t]\n",
        "          mat_gbar_decay:\n",
        "          mat_gbar_weight:  Used to update mat_gbar:\n",
        "               mat_gbar_j[t] = mat_gbar_decay[t] * mat_gbar_j[t-1]\n",
        "                               + mat_gbar_weight[t] * gg_j[t]\n",
        "          learning_rate: Similar to SGD\n",
        "          svd_interval: We should do SVD after this many steps. Default = 1, i.e.\n",
        "                        every step. Usually 20 leads to no loss of accuracy, and\n",
        "                        50 or 100 is also OK. May also want more often early,\n",
        "                        and less often later - set in caller as for example:\n",
        "                        \"svd_interval = lambda(T): tf.cond(\n",
        "                            T < 2000, lambda: 20.0, lambda: 1000.0)\"\n",
        "          precond_update_interval: We should update the preconditioners after\n",
        "                                   this many steps. Default = 1. Usually less than\n",
        "                                   svd_interval.\n",
        "          epsilon:  epsilon * I_n is added to each mat_gbar_j for stability for\n",
        "                    non-diagonal version of shampoo.\n",
        "          alpha:  total power of the preconditioners.\n",
        "          use_iterative_root: should the optimizer use SVD (faster) or the\n",
        "                              iterative root method (for TPU) for finding the\n",
        "                              roots of PSD matrices.\n",
        "          use_locking:\n",
        "          name: name of optimizer.\n",
        "        \"\"\"\n",
        "\n",
        "        super(ShampooOptimizer, self).__init__(use_locking, name)\n",
        "\n",
        "        self._global_step = math_ops.cast(global_step, dtypes.float32)\n",
        "        self._max_matrix_size = max_matrix_size\n",
        "        self._gbar_decay = gbar_decay\n",
        "        self._gbar_weight = gbar_weight\n",
        "        self._mat_gbar_decay = mat_gbar_decay\n",
        "        self._mat_gbar_weight = mat_gbar_weight\n",
        "        self._learning_rate = learning_rate\n",
        "        self._svd_interval = svd_interval\n",
        "        self._precond_update_interval = precond_update_interval\n",
        "        self._epsilon = epsilon\n",
        "        self._alpha = alpha\n",
        "        self._use_iterative_root = use_iterative_root\n",
        "        self._name = name\n",
        "\n",
        "    def _create_slots(self, var_list):\n",
        "        for v in var_list:\n",
        "            with ops.colocate_with(v):\n",
        "                _ = self._zeros_slot(v, \"gbar\", self._name)\n",
        "                shape = np.array(v.get_shape())\n",
        "                for i, d in enumerate(shape):\n",
        "                    d_tensor = ops.convert_to_tensor(d)\n",
        "                    if d <= self._max_matrix_size:\n",
        "                        mat_g_init = array_ops.zeros_like(linalg_ops.eye(d_tensor))\n",
        "                        if self._svd_interval > 1:\n",
        "                            _ = self._get_or_make_slot(v, linalg_ops.eye(d_tensor),\n",
        "                                         \"H_\" + str(i), self._name)\n",
        "                    else:\n",
        "                        mat_g_init = array_ops.zeros([d_tensor])\n",
        "\n",
        "                    _ = self._get_or_make_slot(v, mat_g_init, \"Gbar_\" + str(i),\n",
        "                                     self._name)\n",
        "\n",
        "    def _resource_apply_dense(self, grad, var):\n",
        "        return self._apply_dense(grad, var)\n",
        "\n",
        "    def _apply_dense(self, grad, var):\n",
        "        return self._apply_gradient(grad, var)\n",
        "\n",
        "    def _resource_apply_sparse(self, grad_values, var, grad_indices):\n",
        "        return self._apply_sparse_shared(grad_values, grad_indices, var)\n",
        "\n",
        "    def _apply_sparse(self, grad, var):\n",
        "        return self._apply_sparse_shared(grad.values, grad.indices, var)\n",
        "\n",
        "    def _apply_sparse_shared(self, grad_values, grad_indices, var):\n",
        "        if var.get_shape()[0] <= self._max_matrix_size or self._gbar_decay != 0.0:\n",
        "            # The dimension is small enough, we can make the variable dense and\n",
        "            # do a dense update\n",
        "            dense_grad = array_ops.scatter_nd(\n",
        "            array_ops.expand_dims(grad_indices, axis=1), grad_values,\n",
        "            array_ops.shape(var, out_type=grad_indices.dtype))\n",
        "            return self._apply_gradient(dense_grad, var)\n",
        "    \n",
        "        return self._apply_gradient(grad_values, var, grad_indices)\n",
        "\n",
        "    def _weighted_average(self, var, weight, weight_t, rest):\n",
        "        \"\"\"Computes exponential weighted average: var = weight_t * var + rest.\n",
        "\n",
        "        Important to ensure that var does not occur in rest, otherwise\n",
        "        we can get race conditions in a distributed setting.\n",
        "\n",
        "        Args:\n",
        "          var: variable to be updated\n",
        "          weight: parameter to be checked. If it is a constant, we can optimize.\n",
        "          weight_t: current value of parameter, used for weighting\n",
        "          rest: the remaining tensor to be added\n",
        "\n",
        "        Returns:\n",
        "          updated variable.\n",
        "        \"\"\"\n",
        "        if weight == 0.0:\n",
        "            return rest       # no need to update var, we will never use it.\n",
        "        \n",
        "        if weight == 1.0:   # common case\n",
        "            return state_ops.assign_add(var, rest)\n",
        "        # The op below can cause race conditions in a distributed setting,\n",
        "        # since computing weight_t * var + rest can take some time, during\n",
        "        # which var may be set by another worker. To prevent this, it should\n",
        "        # be implemented as a C++ op.\n",
        "        return var.assign_add((weight_t - 1) * var + rest)\n",
        "\n",
        "    def _update_mat_g(self, mat_g, grad, axes, mat_gbar_decay,\n",
        "                    mat_gbar_weight, i):\n",
        "        \"\"\"Updates the cumulative outer products of the gradients.\n",
        "\n",
        "        Args:\n",
        "          mat_g: the matrix to be updated\n",
        "          grad: the gradient of the variable\n",
        "          axes: a list of k-1 integers 0 to k-1, except i\n",
        "          mat_gbar_decay: constant for weighted average:\n",
        "              mat_g = mat_g * decay + grad * weight\n",
        "          mat_gbar_weight: constant for weighted average\n",
        "          i: index of dimension to be updated.\n",
        "\n",
        "        Returns:\n",
        "          updated mat_g = mat_g * mat_gbar_decay + grad_outer * mat_gbar_weight\n",
        "\n",
        "        In Einstein notation if i = 0: grad_outer_aa'= g_abcd g_a'bcd\n",
        "        thus grad_outer is a matrix d_i x d_i, where d_i is the size of the\n",
        "        i'th dimension of g.\n",
        "        Alternate view: If mat_i(grad) is the flattening of grad to a\n",
        "        d_i x (d_1d_2...d_{i-1}d_{i+1}...d_k) matrix, then\n",
        "             grad_outer = mat_i(grad) mat_i(grad).transpose\n",
        "        \"\"\"\n",
        "        grad_outer = math_ops.tensordot(grad, grad, axes=(axes, axes),\n",
        "                                        name=\"grad_outer_\" + str(i))\n",
        "        return self._weighted_average(mat_g, self._mat_gbar_decay, mat_gbar_decay,\n",
        "                                      mat_gbar_weight * grad_outer)\n",
        "\n",
        "    def _compute_power_svd(self, var, mat_g, mat_g_size, alpha, mat_h_slot_name):\n",
        "        \"\"\"Computes mat_h = mat_g^alpha using svd. mat_g is a symmetric PSD matrix.\n",
        "\n",
        "        Args:\n",
        "          var: the variable we are updating.\n",
        "          mat_g: the symmetric PSD matrix whose power it to be computed\n",
        "          mat_g_size: size of mat_g\n",
        "          alpha: a real number\n",
        "          mat_h_slot_name: name of slot to store the power, if needed.\n",
        "\n",
        "        Returns:\n",
        "          mat_h = mat_g^alpha\n",
        "\n",
        "        Stores mat_h in the appropriate slot, if it exists.\n",
        "        Note that mat_g is PSD. So we could use linalg_ops.self_adjoint_eig.\n",
        "        \"\"\"\n",
        "        if mat_g_size == 1:\n",
        "            mat_h = math_ops.pow(mat_g + self._epsilon, alpha)\n",
        "        else:\n",
        "            damping = self._epsilon * linalg_ops.eye(\n",
        "              math_ops.cast(mat_g_size, dtypes.int32))\n",
        "            diag_d, mat_u, mat_v = linalg_ops.svd(mat_g + damping, full_matrices=True)\n",
        "            mat_h = math_ops.matmul(\n",
        "              mat_v * math_ops.pow(math_ops.maximum(diag_d, self._epsilon), alpha),\n",
        "              array_ops.transpose(mat_u))\n",
        "        if mat_h_slot_name is not None:\n",
        "            return state_ops.assign(self.get_slot(var, mat_h_slot_name), mat_h)\n",
        "        return mat_h\n",
        "\n",
        "    def _compute_power_iter(self, var, mat_g, mat_g_size, alpha, mat_h_slot_name,\n",
        "                          iter_count=100, epsilon=1e-6):\n",
        "        \"\"\"Computes mat_g^alpha, where alpha = -1/p, p a positive integer.\"\"\"\n",
        "\n",
        "        mat_g_sqrt = matrix_functions.matrix_square_root(mat_g, mat_g_size,\n",
        "                                                         iter_count, self._epsilon)\n",
        "        mat_h = matrix_functions.matrix_inverse_pth_root(\n",
        "            mat_g_sqrt,\n",
        "            mat_g_size,\n",
        "            2 * alpha,\n",
        "            iter_count,\n",
        "            epsilon,\n",
        "            ridge_epsilon=0.0)\n",
        "\n",
        "        if mat_h_slot_name is not None:\n",
        "            return state_ops.assign(self.get_slot(var, mat_h_slot_name), mat_h)\n",
        "        return mat_h\n",
        "\n",
        "    def _compute_power(self, var, mat_g, mat_g_size, alpha, mat_h_slot_name=None):\n",
        "        \"\"\"Just a switch between the iterative power vs svd.\"\"\"\n",
        "        with ops.name_scope(\"matrix_iterative_power\"):\n",
        "            if self._use_iterative_root:\n",
        "                return self._compute_power_iter(var, mat_g, mat_g_size, alpha,\n",
        "                                            mat_h_slot_name)\n",
        "            else:\n",
        "                return self._compute_power_svd(var, mat_g, mat_g_size, alpha,\n",
        "                                           mat_h_slot_name)\n",
        "\n",
        "    def _apply_gradient(self, grad, var, indices=None):\n",
        "        \"\"\"The main function to update a variable.\n",
        "\n",
        "        Args:\n",
        "          grad: A Tensor containing gradient to apply.\n",
        "          var: A Tensor containing the variable to update.\n",
        "          indices: An array of integers, for sparse update.\n",
        "\n",
        "        Returns:\n",
        "          Updated variable var = var - learning_rate * preconditioner * grad\n",
        "\n",
        "        If the gradient is dense, var and grad have the same shape.\n",
        "        If the update is sparse, then the first dimension of the gradient and var\n",
        "        may differ, others are all the same. In this case the indices array\n",
        "        provides the set of indices of the variable which are to be updated with\n",
        "        each row of the gradient.\n",
        "        \"\"\"\n",
        "        global_step = self._global_step + 1\n",
        "\n",
        "        # Update accumulated weighted average of gradients\n",
        "        gbar = self.get_slot(var, \"gbar\")\n",
        "        gbar_decay_t = GetParam(self._gbar_decay, global_step)\n",
        "        gbar_weight_t = GetParam(self._gbar_weight, global_step)\n",
        "        if indices is not None:\n",
        "          # Note - the sparse update is not easily implemented, since the\n",
        "          # algorithm needs all indices of gbar to be updated\n",
        "          # if mat_gbar_decay != 1 or mat_gbar_decay != 0.\n",
        "          # One way to make mat_gbar_decay = 1 is by rescaling.\n",
        "          # If we want the update:\n",
        "          #         G_{t+1} = a_{t+1} G_t + b_{t+1} w_t\n",
        "          # define:\n",
        "          #         r_{t+1} = a_{t+1} * r_t\n",
        "          #         h_t = G_t / r_t\n",
        "          # Then:\n",
        "          #         h_{t+1} = h_t + (b_{t+1} / r_{t+1}) * w_t\n",
        "          # So we get the mat_gbar_decay = 1 as desired.\n",
        "          # We can implement this in a future version as needed.\n",
        "          # However we still need gbar_decay = 0, otherwise all indices\n",
        "          # of the variable will need to be updated.\n",
        "            if self._gbar_decay != 0.0:\n",
        "                tf_logging.warning(\"Not applying momentum for variable: %s\" % var.name)\n",
        "            gbar_updated = grad\n",
        "        else:\n",
        "            gbar_updated = self._weighted_average(gbar, self._gbar_decay,\n",
        "                                                gbar_decay_t,\n",
        "                                                gbar_weight_t * grad)\n",
        "\n",
        "        # Update the preconditioners and compute the preconditioned gradient\n",
        "        shape = var.get_shape()\n",
        "        mat_g_list = []\n",
        "        for i in range(len(shape)):\n",
        "            mat_g_list.append(self.get_slot(var, \"Gbar_\" + str(i)))\n",
        "        mat_gbar_decay_t = GetParam(self._mat_gbar_decay, global_step)\n",
        "        mat_gbar_weight_t = GetParam(self._mat_gbar_weight, global_step)\n",
        "\n",
        "        preconditioned_grad = gbar_updated\n",
        "        v_rank = len(mat_g_list)\n",
        "        neg_alpha = - GetParam(self._alpha, global_step) / v_rank\n",
        "        svd_interval = GetParam(self._svd_interval, global_step)\n",
        "        precond_update_interval = GetParam(self._precond_update_interval,\n",
        "                                           global_step)\n",
        "        for i, mat_g in enumerate(mat_g_list):\n",
        "            # axes is the list of indices to reduce - everything but the current i.\n",
        "            axes = list(range(i)) + list(range(i+1, v_rank))\n",
        "            if shape[i] <= self._max_matrix_size:\n",
        "                # If the tensor size is sufficiently small perform full Shampoo update\n",
        "                # Note if precond_update_interval > 1 and mat_gbar_decay_t != 1, this\n",
        "                # is not strictly correct. However we will use it for now, and\n",
        "                # fix if needed. (G_1 = aG + bg ==> G_n = a^n G + (1+a+..+a^{n-1})bg)\n",
        "\n",
        "                # pylint: disable=g-long-lambda,cell-var-from-loop\n",
        "                mat_g_updated = control_flow_ops.cond(\n",
        "                    math_ops.mod(global_step, precond_update_interval) < 1,\n",
        "                    lambda: self._update_mat_g(mat_g, grad, axes, mat_gbar_decay_t,\n",
        "                                           mat_gbar_weight_t * precond_update_interval, i), lambda: mat_g)\n",
        "\n",
        "                #БЫЛО ТАК: mat_g_updated = mat_g_updated / float(shape[i].value)\n",
        "                mat_g_updated = mat_g_updated / float(shape[i])\n",
        "\n",
        "                if self._svd_interval == 1:\n",
        "                    mat_h = self._compute_power(var, mat_g_updated, shape[i], neg_alpha)\n",
        "                else:\n",
        "                    mat_h = control_flow_ops.cond(\n",
        "                        math_ops.mod(global_step, svd_interval) < 1,\n",
        "                        lambda: self._compute_power(var, mat_g_updated, shape[i],\n",
        "                                              neg_alpha, \"H_\" + str(i)),\n",
        "                        lambda: self.get_slot(var, \"H_\" + str(i)))\n",
        "\n",
        "                # mat_h is a square matrix of size d_i x d_i\n",
        "                # preconditioned_grad is a d_i x ... x d_n x d_0 x ... d_{i-1} tensor\n",
        "                # After contraction with a d_i x d_i tensor\n",
        "                # it becomes a d_{i+1} x ... x d_n x d_0 x ... d_i tensor\n",
        "                # (the first dimension is contracted out, and the second dimension of\n",
        "                # mat_h is appended).  After going through all the indices, it becomes\n",
        "                # a d_0 x ... x d_n tensor again.\n",
        "                preconditioned_grad = math_ops.tensordot(preconditioned_grad, mat_h,\n",
        "                                                         axes=([0], [0]),\n",
        "                                                         name=\"precond_\" + str(i))\n",
        "            else:\n",
        "                # Tensor size is too large -- perform diagonal Shampoo update\n",
        "                # Only normalize non-vector cases.\n",
        "                if axes:\n",
        "                    #БЫЛО ТАК: normalizer = 1.0 if indices is not None else float(shape[i].value)\n",
        "                    normalizer = 1.0 if indices is not None else float(shape[i])\n",
        "                    grad_outer = math_ops.reduce_sum(grad * grad, axis=axes) / normalizer\n",
        "                else:\n",
        "                    grad_outer = grad * grad\n",
        "\n",
        "                if i == 0 and indices is not None:\n",
        "                    assert self._mat_gbar_decay == 1.0\n",
        "                    mat_g_updated = state_ops.scatter_add(mat_g, indices,\n",
        "                                                        mat_gbar_weight_t * grad_outer)\n",
        "                    mat_g_updated_slice = array_ops.gather(mat_g_updated, indices)\n",
        "                    mat_h = array_ops.where(\n",
        "                      math_ops.greater(mat_g_updated_slice, 0),\n",
        "                      math_ops.pow(mat_g_updated_slice, neg_alpha),\n",
        "                      array_ops.zeros_like(mat_g_updated_slice))\n",
        "                else:\n",
        "                    mat_g_updated = self._weighted_average(mat_g,\n",
        "                                                         self._mat_gbar_decay,\n",
        "                                                         mat_gbar_decay_t,\n",
        "                                                         mat_gbar_weight_t * grad_outer)\n",
        "                    mat_h = array_ops.where(\n",
        "                      math_ops.greater(mat_g_updated, 0),\n",
        "                      math_ops.pow(mat_g_updated, neg_alpha),\n",
        "                      array_ops.zeros_like(mat_g_updated))\n",
        "\n",
        "                # Need to do the transpose to ensure that the tensor becomes\n",
        "                # a d_{i+1} x ... x d_n x d_0 x ... d_i tensor as described above.\n",
        "                preconditioned_grad = array_ops.transpose(\n",
        "                    preconditioned_grad, perm=list(range(1, v_rank)) + [0]) * mat_h\n",
        "\n",
        "        # Update the variable based on the Shampoo update\n",
        "        learning_rate_t = GetParam(self._learning_rate, global_step)\n",
        "        if indices is not None:\n",
        "            var_updated = state_ops.scatter_add(\n",
        "              var, indices, -learning_rate_t * preconditioned_grad)\n",
        "        else:\n",
        "            var_updated = state_ops.assign_sub(var,\n",
        "                                             learning_rate_t * preconditioned_grad)\n",
        "        return var_updated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV-THAovnJXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import datasets, layers, models\n",
        "import tensorflow as tf\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation, Dropout\n",
        "from keras.layers import AveragePooling2D, Input, Flatten, GlobalAveragePooling2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model, Sequential\n",
        "from keras.datasets import cifar10\n",
        "from keras import backend\n",
        "from keras.utils import np_utils\n",
        "import numpy\n",
        "from keras.datasets import cifar10\n",
        "from keras.layers import Flatten\n",
        "from keras.constraints import maxnorm\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9JoJ_9hnJX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import mnist\n",
        "#download mnist data and split into train and test sets\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2K7Twx8JnJYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(60000,28,28,1)\n",
        "X_test = X_test.reshape(10000,28,28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsEFBsSrnJY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
        "#create model\n",
        "model = Sequential()\n",
        "#add model layers\n",
        "model.add(Conv2D(64, kernel_size=3, activation=\"relu\", input_shape=(28,28,1)))\n",
        "model.add(Conv2D(32, kernel_size=3, activation=\"relu\"))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation=\"softmax\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTjoKP2VnJY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpEBC-Q8nJZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(model, x, y, loss_object):\n",
        "    y_ = model(x)\n",
        "\n",
        "    return loss_object(y_true=y, y_pred=y_)\n",
        "\n",
        "def grad(model, inputs, targets, loss_object):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss_value = loss(model, inputs, targets, loss_object)\n",
        "        return loss_value, tape.gradient(loss_value, model.trainable_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu2xKde8nJZ4",
        "colab_type": "code",
        "outputId": "c99be6fd-b4cb-48dd-e6b5-d905c2379019",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.001)\n",
        "# optimizer = ShampooOptimizer(learning_rate=0.0001)\n",
        "\n",
        "adam_train_loss_results = []\n",
        "adam_train_accuracy_results = []\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 128\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "    epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "    # Training loop - using batches of 128\n",
        "    for i in range(X_train.shape[0] // batch_size):\n",
        "        x = X_train[i*batch_size:i*batch_size+batch_size]\n",
        "        y = y_train[i*batch_size:i*batch_size+batch_size]\n",
        "        # Optimize the model\n",
        "        loss_value, grads = grad(model, x, y, loss_object)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "        # Track progress\n",
        "        epoch_loss_avg(loss_value)  # Add current batch loss\n",
        "        # Compare predicted label to actual label\n",
        "        epoch_accuracy(y, model(x))\n",
        "\n",
        "        # End epoch\n",
        "    adam_train_loss_results.append(epoch_loss_avg.result())\n",
        "    adam_train_accuracy_results.append(epoch_accuracy.result())\n",
        "\n",
        "    print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,\n",
        "                                                                epoch_loss_avg.result(),\n",
        "                                                                epoch_accuracy.result()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 000: Loss: 1.826, Accuracy: 65.042%\n",
            "Epoch 001: Loss: 1.703, Accuracy: 76.833%\n",
            "Epoch 002: Loss: 1.692, Accuracy: 77.571%\n",
            "Epoch 003: Loss: 1.686, Accuracy: 77.995%\n",
            "Epoch 004: Loss: 1.682, Accuracy: 78.349%\n",
            "Epoch 005: Loss: 1.679, Accuracy: 78.617%\n",
            "Epoch 006: Loss: 1.676, Accuracy: 78.829%\n",
            "Epoch 007: Loss: 1.674, Accuracy: 78.971%\n",
            "Epoch 008: Loss: 1.672, Accuracy: 79.132%\n",
            "Epoch 009: Loss: 1.671, Accuracy: 79.235%\n",
            "Epoch 010: Loss: 1.669, Accuracy: 79.372%\n",
            "Epoch 011: Loss: 1.662, Accuracy: 80.162%\n",
            "Epoch 012: Loss: 1.587, Accuracy: 88.014%\n",
            "Epoch 013: Loss: 1.579, Accuracy: 88.674%\n",
            "Epoch 014: Loss: 1.576, Accuracy: 88.879%\n",
            "Epoch 015: Loss: 1.573, Accuracy: 89.071%\n",
            "Epoch 016: Loss: 1.572, Accuracy: 89.208%\n",
            "Epoch 017: Loss: 1.570, Accuracy: 89.293%\n",
            "Epoch 018: Loss: 1.569, Accuracy: 89.378%\n",
            "Epoch 019: Loss: 1.568, Accuracy: 89.456%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qovxVkNn_0m",
        "colab_type": "code",
        "outputId": "68781129-85fc-4d6a-f1dc-31c11dc4c27c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "print(\"MNIST, Adagrad, lr = 10e-4, loss = {0}\".format(np.array(adam_train_loss_results)))\n",
        "print(\"MNIST, Adagrad, lr = 10e-4, loss = {0}\".format(np.array(adam_train_accuracy_results)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MNIST, Adagrad, lr = 10e-4, loss = [1.8257794 1.7025895 1.6923213 1.686369  1.6823964 1.6788954 1.676232\n",
            " 1.6741681 1.672348  1.670665  1.6692    1.6620312 1.5872949 1.5789883\n",
            " 1.5756624 1.5734642 1.5718085 1.5703863 1.5692716 1.5683758]\n",
            "MNIST, Adagrad, lr = 10e-4, loss = [0.650424   0.7683293  0.7757078  0.77994794 0.7834869  0.78617454\n",
            " 0.7882946  0.78971356 0.7913161  0.79235107 0.79371995 0.8016159\n",
            " 0.88014156 0.88673544 0.8887887  0.89070845 0.8920773  0.89292866\n",
            " 0.89378005 0.8945646 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvYDqjpP0Ukk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}